name: Scrape AFDJ (Selenium - Simple)

on:
  # Doar manual
  workflow_dispatch:

permissions:
  contents: read  # Doar citire, fƒÉrƒÉ push

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    
    - name: Install Chrome
      run: |
        wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
        sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google-chrome.list'
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests beautifulsoup4 pandas openpyxl lxml selenium undetected-chromedriver
    
    - name: Run Selenium scraper (with retries)
      run: |
        python -c "
        from afdj_selenium_scraper import AFDJSeleniumScraper
        import time
        
        # √éncearcƒÉ de 3 ori
        success = False
        for attempt in range(3):
            print(f'\nüîÑ Attempt {attempt + 1}/3')
            try:
                scraper = AFDJSeleniumScraper(headless=True)
                data = scraper.scrape(save_screenshot=True)
                
                if data and len(data) > 0:
                    print(f'‚úÖ SUCCESS! Scraped {len(data)} ports')
                    success = True
                    break
                else:
                    print('‚ö†Ô∏è  No data found, waiting before retry...')
                    if attempt < 2:
                        time.sleep(10)
            except Exception as e:
                print(f'‚ùå Error: {e}')
                if attempt < 2:
                    print('Waiting 15 seconds before retry...')
                    time.sleep(15)
        
        if not success:
            print('\n‚ùå All attempts failed!')
            print('AFDJ might be blocking GitHub Actions IPs.')
            print('Try running the scraper locally on your computer.')
        "
    
    - name: Upload results (if any)
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: selenium-results-${{ github.run_number }}
        path: |
          cote_selenium.json
          cote_selenium.csv
          afdj_page.png
        retention-days: 30
        if-no-files-found: ignore
    
    - name: Display summary
      if: always()
      run: |
        echo "## üåä Selenium Scraping Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ -f cote_selenium.json ]; then
          PORTS=$(python -c "import json; data=json.load(open('cote_selenium.json')); print(data['count'])" 2>/dev/null || echo "0")
          echo "‚úÖ **Success!** Scraped $PORTS ports" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Files created:" >> $GITHUB_STEP_SUMMARY
          ls -lh cote_selenium.* 2>/dev/null | awk '{print "- " $9 " (" $5 ")"}' >> $GITHUB_STEP_SUMMARY
        else
          echo "‚ùå **Scraping failed**" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Possible reasons:" >> $GITHUB_STEP_SUMMARY
          echo "- AFDJ is blocking GitHub Actions" >> $GITHUB_STEP_SUMMARY
          echo "- Cloudflare detected automation" >> $GITHUB_STEP_SUMMARY
          echo "- Site structure changed" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "üí° **Try running locally:** \`python afdj_selenium_scraper.py\`" >> $GITHUB_STEP_SUMMARY
        fi
