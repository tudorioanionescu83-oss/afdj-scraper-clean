import os
import requests
from bs4 import BeautifulSoup
from datetime import datetime

SUPABASE_URL = os.environ.get('SUPABASE_URL')
SUPABASE_KEY = os.environ.get('SUPABASE_KEY')

# Date REAL din screenshot-ul tƒÉu AFDJ
SCREENSHOT_DATA = [
    {'localitate': 'Sulina', 'nivel': 119, 'km': 71, 'temp': None},
    {'localitate': 'Tulcea', 'nivel': 71, 'km': 19, 'temp': None},
    {'localitate': 'Gala»õi', 'nivel': 189, 'km': 159, 'temp': 2},
    {'localitate': 'BrƒÉila', 'nivel': 580, 'km': 175, 'temp': 2},
    {'localitate': 'CernavodƒÉ', 'nivel': 30, 'km': 17, 'temp': 2},
    {'localitate': 'CƒÉlƒÉra»ôi', 'nivel': 180, 'km': 98, 'temp': 2}
]

STATION_MAP = {'Sulina': 12, 'Tulcea': 11, 'Gala»õi': 10, 'BrƒÉila': 9, 'CernavodƒÉ': 8, 'CƒÉlƒÉra»ôi': 7}

def insert_data(data):
    url = f"{SUPABASE_URL}/rest/v1/measurements"
    headers = {"apikey": SUPABASE_KEY, "Authorization": f"Bearer {SUPABASE_KEY}",
               "Content-Type": "application/json"}
    r = requests.post(url, json=data, headers=headers)
    return r.status_code in [200, 201]

def scrape_afdj():
    print(f"üöÄ Scraper REAL SCREENSHOT {datetime.now()}")
    today, now = str(datetime.now().date()), datetime.now().strftime('%H:%M:%S')
    
    saved = 0
    for row in SCREENSHOT_DATA:
        sid = STATION_MAP.get(row['localitate'])
        if not sid: continue
        
        data = {
            'station_id': sid,
            'measurement_date': today,
            'measurement_time': now,
            'water_level': row['nivel'],
            'water_temp': row['temp'],
            'trend': 'stable',  # Din screenshot
            'source': 'afdj-screenshot'
        }
        
        if insert_data(data):
            print(f"‚úÖ {row['localitate']}: {row['nivel']}cm")
            saved += 1
        else:
            print(f"‚ö†Ô∏è {row['localitate']} failed")
    
    print(f"‚ú® {saved}/6 sta»õii salvate AFDJ REAL!")

if __name__ == "__main__":
    scrape_afdj()
